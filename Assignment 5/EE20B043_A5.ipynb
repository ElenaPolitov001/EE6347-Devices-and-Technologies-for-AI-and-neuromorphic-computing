{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transforms to preprocess the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Download the MNIST datasets\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 100\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "d1WxjPJj10nm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf8a9f3-b233-43cb-a732-79a1880c2c4c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 34319844.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 81903782.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 64066878.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 13559095.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = Net()\n"
      ],
      "metadata": {
        "id": "mmg07mVM12Au"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "def test(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, criterion, optimizer, num_epochs=25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUDZBvNG16YO",
        "outputId": "9012dfb4-eec5-4bc1-88ec-d9767e1ca2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.2929398519794146\n",
            "Epoch 2, Loss: 0.465682396988074\n",
            "Epoch 3, Loss: 0.37238926728566485\n",
            "Epoch 4, Loss: 0.3326904684801896\n",
            "Epoch 5, Loss: 0.3063545715312163\n",
            "Epoch 6, Loss: 0.2851218855753541\n",
            "Epoch 7, Loss: 0.2669487547377745\n",
            "Epoch 8, Loss: 0.2504315092911323\n",
            "Epoch 9, Loss: 0.23541991130759318\n",
            "Epoch 10, Loss: 0.22135278141126036\n",
            "Epoch 11, Loss: 0.2083909743403395\n",
            "Epoch 12, Loss: 0.19651292011141777\n",
            "Epoch 13, Loss: 0.18562815137207508\n",
            "Epoch 14, Loss: 0.17598263083025814\n",
            "Epoch 15, Loss: 0.16691079222907623\n",
            "Epoch 16, Loss: 0.15913317969689766\n",
            "Epoch 17, Loss: 0.15181762443855404\n",
            "Epoch 18, Loss: 0.14463093611722191\n",
            "Epoch 19, Loss: 0.13837042587498824\n",
            "Epoch 20, Loss: 0.13277095414698123\n",
            "Epoch 21, Loss: 0.12683287946507335\n",
            "Epoch 22, Loss: 0.12232867147152622\n",
            "Epoch 23, Loss: 0.1177519887375335\n",
            "Epoch 24, Loss: 0.11299941732548177\n",
            "Epoch 25, Loss: 0.10899771963556608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_accuracy = test(model, test_loader)\n",
        "print(f\"Final Validation Accuracy: {final_accuracy}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3HKIY0y17z-",
        "outputId": "6f94a5cb-e7ae-4436-8cb3-fb60537a049d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Accuracy: 96.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aihwkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYnVHXcn76uE",
        "outputId": "c9266449-8828-43b9-973c-46c6fe8f0340"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aihwkit\n",
            "  Downloading aihwkit-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.0.1 (from aihwkit)\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from aihwkit) (0.16.0+cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from aihwkit) (1.11.3)\n",
            "Requirement already satisfied: requests<3,>=2.25 in /usr/local/lib/python3.10/dist-packages (from aihwkit) (2.31.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from aihwkit) (1.23.5)\n",
            "Collecting protobuf>=4.21.6 (from aihwkit)\n",
            "  Downloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.4/294.4 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->aihwkit) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->aihwkit) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->aihwkit) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->aihwkit) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->aihwkit) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->aihwkit)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1->aihwkit)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->aihwkit) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->aihwkit) (0.41.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->aihwkit) (3.27.7)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1->aihwkit)\n",
            "  Downloading lit-17.0.4.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.25->aihwkit) (2023.7.22)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from aihwkit)\n",
            "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->aihwkit) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->aihwkit) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->aihwkit) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.4-py3-none-any.whl size=93257 sha256=8d47663b4f5e2efb903a7e9068af5506f435d0ea72dd86acf9338418f56e2f2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/ae/00/696c57d438bfc7c0e89c4c379083ea08b1c2e54d85a5f7cd7c\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, protobuf, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchvision, aihwkit\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu118\n",
            "    Uninstalling torchvision-0.16.0+cu118:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aihwkit-0.8.0 lit-17.0.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 protobuf-4.25.0 torch-2.0.1 torchvision-0.15.2 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aihwkit.nn import AnalogLinear\n",
        "from aihwkit.optim import AnalogSGD\n",
        "from aihwkit.simulator.configs import SingleRPUConfig\n",
        "from aihwkit.simulator.configs.devices import ConstantStepDevice\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the custom neural network with AnalogLinear layers\n",
        "class AnalogNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AnalogNet, self).__init__()\n",
        "        self.config = SingleRPUConfig(device=ConstantStepDevice(w_min=-0.4))\n",
        "        self.fc1 = AnalogLinear(28 * 28, 128)  # Replace Linear with AnalogLinear\n",
        "        self.fc2 = AnalogLinear(128, 64)     # Replace Linear with AnalogLinear\n",
        "        self.fc3 = AnalogLinear(64, 10)      # Replace Linear with AnalogLinear\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iEsK-E1I9X1I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model_const = AnalogNet()\n",
        "\n",
        "# Apply regroup_param_groups to the model parameters (required by the tool)\n",
        "analog_optimizer_const = AnalogSGD(model_const.parameters(),lr=0.01)\n",
        "analog_optimizer_const.regroup_param_groups(model_const)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Train the model\n",
        "train(model_const, train_loader, criterion, analog_optimizer_const, num_epochs=25)"
      ],
      "metadata": {
        "id": "YpB9Qc6h_Hhx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb56c34-c34a-4e48-ba43-0ab3d00b58ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.3792122376461824\n",
            "Epoch 2, Loss: 0.5321631866693497\n",
            "Epoch 3, Loss: 0.45754598346849285\n",
            "Epoch 4, Loss: 0.4273408761372169\n",
            "Epoch 5, Loss: 0.42180571280419826\n",
            "Epoch 6, Loss: 0.5050675835212072\n",
            "Epoch 7, Loss: 0.615622464766105\n",
            "Epoch 8, Loss: 0.6494255693505208\n",
            "Epoch 9, Loss: 0.6324662974228462\n",
            "Epoch 10, Loss: 0.6311758956561486\n",
            "Epoch 11, Loss: 0.6379850436747074\n",
            "Epoch 12, Loss: 0.6382508234431347\n",
            "Epoch 13, Loss: 0.6468948687613011\n",
            "Epoch 14, Loss: 0.6456993545095125\n",
            "Epoch 15, Loss: 0.6525525434563557\n",
            "Epoch 16, Loss: 0.6671155874431133\n",
            "Epoch 17, Loss: 0.6657195872813463\n",
            "Epoch 18, Loss: 0.6694677875687679\n",
            "Epoch 19, Loss: 0.6741464104751745\n",
            "Epoch 20, Loss: 0.669849790285031\n",
            "Epoch 21, Loss: 0.6790625722954671\n",
            "Epoch 22, Loss: 0.6810973875224591\n",
            "Epoch 23, Loss: 0.6790054265906413\n",
            "Epoch 24, Loss: 0.6874910006672144\n",
            "Epoch 25, Loss: 0.6786023711164793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model and report the final accuracy\n",
        "def test(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "final_accuracy_const = test(model_const, test_loader)\n",
        "print(f\"Final Validation Accuracy: {final_accuracy_const}%\")"
      ],
      "metadata": {
        "id": "EDtNUhmY_NMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b8bac4b-7121-4ab0-fe22-ab04e7fa8884"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Accuracy: 84.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(ReRamESPresetDevice())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgw3bu5FcOZW",
        "outputId": "fe39fe8d-7130-445a-aa1f-884b29eafaf4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on ReRamESPresetDevice in module aihwkit.simulator.presets.devices object:\n",
            "\n",
            "class ReRamESPresetDevice(aihwkit.simulator.configs.devices.ExpStepDevice)\n",
            " |  ReRamESPresetDevice(construction_seed: int = 0, corrupt_devices_prob: float = 0.0, corrupt_devices_range: float = 0.1, diffusion: float = 0.0, diffusion_dtod: float = 0.0, drift: aihwkit.simulator.parameters.utils.DriftParameter = <factory>, dw_min: float = 0.00135, dw_min_dtod: float = 0.2, dw_min_dtod_log_normal: bool = False, dw_min_std: float = 5.0, enforce_consistency: bool = True, lifetime: float = 0.0, lifetime_dtod: float = 0.0, perfect_bias: bool = False, reset: float = 0.0, reset_dtod: float = 0.0, reset_std: float = 0.01, up_down: float = 0.259359, up_down_dtod: float = 0.05, w_max: float = 1.0, w_max_dtod: float = 0.3, w_min: float = -1.0, w_min_dtod: float = 0.3, count_pulses: bool = False, A_up: float = -1.18445, A_down: float = -0.081404, gamma_up: float = 5.0, gamma_down: float = 5.0, a: float = -0.5, b: float = -0.5, dw_min_std_add: float = 0.0, dw_min_std_slope: float = 0.0, write_noise_std: float = 75.0, apply_write_noise_on_set: bool = True) -> None\n",
            " |  \n",
            " |  Preset configuration for a single RRAM analog resistive processing\n",
            " |  unit based on exp. step device.\n",
            " |  \n",
            " |  Fit of the model :class:`ExpStepDevice` to  `Gong & al., Nat. Commun., 2018`_.\n",
            " |  \n",
            " |  .. _`Gong & al., Nat. Commun., 2018`: https://www.nature.com/articles/s41467-018-04485-1\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      ReRamESPresetDevice\n",
            " |      aihwkit.simulator.configs.devices.ExpStepDevice\n",
            " |      aihwkit.simulator.configs.devices.PulsedDevice\n",
            " |      aihwkit.simulator.parameters.helpers._PrintableMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __eq__(self, other)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __init__(self, construction_seed: int = 0, corrupt_devices_prob: float = 0.0, corrupt_devices_range: float = 0.1, diffusion: float = 0.0, diffusion_dtod: float = 0.0, drift: aihwkit.simulator.parameters.utils.DriftParameter = <factory>, dw_min: float = 0.00135, dw_min_dtod: float = 0.2, dw_min_dtod_log_normal: bool = False, dw_min_std: float = 5.0, enforce_consistency: bool = True, lifetime: float = 0.0, lifetime_dtod: float = 0.0, perfect_bias: bool = False, reset: float = 0.0, reset_dtod: float = 0.0, reset_std: float = 0.01, up_down: float = 0.259359, up_down_dtod: float = 0.05, w_max: float = 1.0, w_max_dtod: float = 0.3, w_min: float = -1.0, w_min_dtod: float = 0.3, count_pulses: bool = False, A_up: float = -1.18445, A_down: float = -0.081404, gamma_up: float = 5.0, gamma_down: float = 5.0, a: float = -0.5, b: float = -0.5, dw_min_std_add: float = 0.0, dw_min_std_slope: float = 0.0, write_noise_std: float = 75.0, apply_write_noise_on_set: bool = True) -> None\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  A_down = -0.081404\n",
            " |  \n",
            " |  A_up = -1.18445\n",
            " |  \n",
            " |  __annotations__ = {'A_down': <class 'float'>, 'A_up': <class 'float'>,...\n",
            " |  \n",
            " |  __dataclass_fields__ = {'A_down': Field(name='A_down',type=<class 'flo...\n",
            " |  \n",
            " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
            " |  \n",
            " |  __hash__ = None\n",
            " |  \n",
            " |  __match_args__ = ('construction_seed', 'corrupt_devices_prob', 'corrup...\n",
            " |  \n",
            " |  __slotnames__ = []\n",
            " |  \n",
            " |  a = -0.5\n",
            " |  \n",
            " |  b = -0.5\n",
            " |  \n",
            " |  dw_min = 0.00135\n",
            " |  \n",
            " |  dw_min_dtod = 0.2\n",
            " |  \n",
            " |  dw_min_std = 5.0\n",
            " |  \n",
            " |  gamma_down = 5.0\n",
            " |  \n",
            " |  gamma_up = 5.0\n",
            " |  \n",
            " |  up_down = 0.259359\n",
            " |  \n",
            " |  up_down_dtod = 0.05\n",
            " |  \n",
            " |  w_max = 1.0\n",
            " |  \n",
            " |  w_max_dtod = 0.3\n",
            " |  \n",
            " |  w_min = -1.0\n",
            " |  \n",
            " |  w_min_dtod = 0.3\n",
            " |  \n",
            " |  write_noise_std = 75.0\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from aihwkit.simulator.configs.devices.ExpStepDevice:\n",
            " |  \n",
            " |  apply_write_noise_on_set = True\n",
            " |  \n",
            " |  bindings_class = <class 'aihwkit.simulator.rpu_base.devices.ExpStepRes...\n",
            " |  \n",
            " |  dw_min_std_add = 0.0\n",
            " |  \n",
            " |  dw_min_std_slope = 0.0\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from aihwkit.simulator.configs.devices.PulsedDevice:\n",
            " |  \n",
            " |  as_bindings(self) -> aihwkit.simulator.rpu_base.devices.PulsedResistiveDeviceParameter\n",
            " |      Return a representation of this instance as a simulator bindings object.\n",
            " |  \n",
            " |  requires_decay(self) -> bool\n",
            " |      Return whether device has decay enabled.\n",
            " |  \n",
            " |  requires_diffusion(self) -> bool\n",
            " |      Return whether device has diffusion enabled.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from aihwkit.simulator.configs.devices.PulsedDevice:\n",
            " |  \n",
            " |  construction_seed = 0\n",
            " |  \n",
            " |  corrupt_devices_prob = 0.0\n",
            " |  \n",
            " |  corrupt_devices_range = 0.1\n",
            " |  \n",
            " |  count_pulses = False\n",
            " |  \n",
            " |  diffusion = 0.0\n",
            " |  \n",
            " |  diffusion_dtod = 0.0\n",
            " |  \n",
            " |  dw_min_dtod_log_normal = False\n",
            " |  \n",
            " |  enforce_consistency = True\n",
            " |  \n",
            " |  lifetime = 0.0\n",
            " |  \n",
            " |  lifetime_dtod = 0.0\n",
            " |  \n",
            " |  perfect_bias = False\n",
            " |  \n",
            " |  reset = 0.0\n",
            " |  \n",
            " |  reset_dtod = 0.0\n",
            " |  \n",
            " |  reset_std = 0.01\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from aihwkit.simulator.parameters.helpers._PrintableMixin:\n",
            " |  \n",
            " |  __str__(self) -> str\n",
            " |      Return a pretty-print representation.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from aihwkit.simulator.parameters.helpers._PrintableMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aihwkit.simulator.presets import ReRamSBPresetDevice,ReRamESPresetDevice,CapacitorPresetDevice, IdealizedPresetDevice,PCMPresetDevice"
      ],
      "metadata": {
        "id": "aHSXQOboIb0p"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PresetNet_Re_SB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PresetNet_Re_SB, self).__init__()\n",
        "        config = SingleRPUConfig(device=ReRamSBPresetDevice(dw_min=0.0005))\n",
        "        self.fc1 = AnalogLinear(28 * 28, 128,rpu_config=config)  # Replace Linear with AnalogLinear\n",
        "        self.fc2 = AnalogLinear(128, 64, rpu_config=config)     # Replace Linear with AnalogLinear\n",
        "        self.fc3 = AnalogLinear(64, 10, rpu_config=config)      # Replace Linear with AnalogLinear\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rrY3hUGqIzhv"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model_re_sb = PresetNet_Re_SB()\n",
        "\n",
        "# Apply regroup_param_groups to the model parameters (required by the tool)\n",
        "analog_optimizer_re_sb = AnalogSGD(model_re_sb.parameters(),lr=0.01)\n",
        "analog_optimizer_re_sb.regroup_param_groups(model_re_sb)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Train the model\n",
        "train(model_re_sb, train_loader, criterion, analog_optimizer_re_sb, num_epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN08lpWcJ6XK",
        "outputId": "c1a669d3-3e0b-4c97-bab2-018338c6da4f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.112740519841512\n",
            "Epoch 2, Loss: 1.348894037604332\n",
            "Epoch 3, Loss: 1.0197316985328992\n",
            "Epoch 4, Loss: 0.877666807572047\n",
            "Epoch 5, Loss: 0.8270255957047145\n",
            "Epoch 6, Loss: 0.7708141126235326\n",
            "Epoch 7, Loss: 0.7512515611449877\n",
            "Epoch 8, Loss: 0.7349214140574137\n",
            "Epoch 9, Loss: 0.7176548382639885\n",
            "Epoch 10, Loss: 0.701624760478735\n",
            "Epoch 11, Loss: 0.6744586074848969\n",
            "Epoch 12, Loss: 0.6706330042084058\n",
            "Epoch 13, Loss: 0.667718369414409\n",
            "Epoch 14, Loss: 0.6539174916346868\n",
            "Epoch 15, Loss: 0.6585526502629121\n",
            "Epoch 16, Loss: 0.6483133211731911\n",
            "Epoch 17, Loss: 0.6359243289629618\n",
            "Epoch 18, Loss: 0.6326238213479519\n",
            "Epoch 19, Loss: 0.6326642408470313\n",
            "Epoch 20, Loss: 0.6211604031423728\n",
            "Epoch 21, Loss: 0.6160159929593404\n",
            "Epoch 22, Loss: 0.610080925822258\n",
            "Epoch 23, Loss: 0.6039068156977495\n",
            "Epoch 24, Loss: 0.61463626096646\n",
            "Epoch 25, Loss: 0.6096699408193429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model and report the final accuracy\n",
        "def test(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "final_accuracy_re_sb = test(model_re_sb, test_loader)\n",
        "print(f\"Final Validation Accuracy: {final_accuracy_re_sb}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1MZj231KlIN",
        "outputId": "08934084-a167-49dc-b700-d6f923a0895b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Accuracy: 80.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PresetNet_Re_ES(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PresetNet_Re_ES, self).__init__()\n",
        "        config = SingleRPUConfig(device=ReRamESPresetDevice(dw_min=0.003))\n",
        "        self.fc1 = AnalogLinear(28 * 28, 128,rpu_config=config)  # Replace Linear with AnalogLinear\n",
        "        self.fc2 = AnalogLinear(128, 64, rpu_config=config)     # Replace Linear with AnalogLinear\n",
        "        self.fc3 = AnalogLinear(64, 10, rpu_config=config)      # Replace Linear with AnalogLinear\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "fSWG7yArNjFF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model_re_es = PresetNet_Re_ES()\n",
        "\n",
        "# Apply regroup_param_groups to the model parameters (required by the tool)\n",
        "analog_optimizer_re_es = AnalogSGD(model_re_es.parameters(),lr=0.01)\n",
        "analog_optimizer_re_es.regroup_param_groups(model_re_es)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Train the model\n",
        "train(model_re_es, train_loader, criterion, analog_optimizer_re_es, num_epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3edViyPRNtY1",
        "outputId": "aa59a052-b482-43f5-e3d8-e171e6a98a1f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.6786723081270853\n",
            "Epoch 2, Loss: 2.313918716907501\n",
            "Epoch 3, Loss: 2.3014424534638724\n",
            "Epoch 4, Loss: 2.301323354244232\n",
            "Epoch 5, Loss: 2.3103964948654174\n",
            "Epoch 6, Loss: 2.3164948149522147\n",
            "Epoch 7, Loss: 2.308799015680949\n",
            "Epoch 8, Loss: 2.3012700502077736\n",
            "Epoch 9, Loss: 2.301670427719752\n",
            "Epoch 10, Loss: 2.3012713102499642\n",
            "Epoch 11, Loss: 2.301224073966344\n",
            "Epoch 12, Loss: 2.303142362833023\n",
            "Epoch 13, Loss: 2.3142498970031737\n",
            "Epoch 14, Loss: 2.3012070628007253\n",
            "Epoch 15, Loss: 2.3011991421381635\n",
            "Epoch 16, Loss: 2.3012215733528136\n",
            "Epoch 17, Loss: 2.3011959783236184\n",
            "Epoch 18, Loss: 2.3021786181132\n",
            "Epoch 19, Loss: 2.3012188732624055\n",
            "Epoch 20, Loss: 2.301214553117752\n",
            "Epoch 21, Loss: 2.301218505303065\n",
            "Epoch 22, Loss: 2.3011983931064606\n",
            "Epoch 23, Loss: 2.3012008325258893\n",
            "Epoch 24, Loss: 2.3012014512221017\n",
            "Epoch 25, Loss: 2.301199638446172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model and report the final accuracy\n",
        "def test(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "final_accuracy_re_es = test(model_re_es, test_loader)\n",
        "print(f\"Final Validation Accuracy: {final_accuracy_re_es}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn5fN7wfOBQ2",
        "outputId": "addaa42b-73bc-4dc4-8c52-24ecdaa61deb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Accuracy: 11.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PresetNet_CapPreDev(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PresetNet_CapPreDev, self).__init__()\n",
        "        config = SingleRPUConfig(device=CapacitorPresetDevice())\n",
        "        self.fc1 = AnalogLinear(28 * 28, 128,rpu_config=config)  # Replace Linear with AnalogLinear\n",
        "        self.fc2 = AnalogLinear(128, 64, rpu_config=config)     # Replace Linear with AnalogLinear\n",
        "        self.fc3 = AnalogLinear(64, 10, rpu_config=config)      # Replace Linear with AnalogLinear\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mvoU7vKzOKv8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model_CapPreDev = PresetNet_CapPreDev()\n",
        "\n",
        "# Apply regroup_param_groups to the model parameters (required by the tool)\n",
        "analog_optimizer_CapPreDev = AnalogSGD(model_CapPreDev.parameters(),lr=0.01)\n",
        "analog_optimizer_CapPreDev.regroup_param_groups(model_CapPreDev)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Train the model\n",
        "train(model_CapPreDev, train_loader, criterion, analog_optimizer_CapPreDev, num_epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEPyxjNIO6Ry",
        "outputId": "41b0105a-9d64-44c1-9d22-d51c6b1026f3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.3855879114568233\n",
            "Epoch 2, Loss: 0.6058886830012004\n",
            "Epoch 3, Loss: 0.6914633496602376\n",
            "Epoch 4, Loss: 0.7654592626790205\n",
            "Epoch 5, Loss: 0.7929244703551134\n",
            "Epoch 6, Loss: 0.8186947929362456\n",
            "Epoch 7, Loss: 0.8375282887617747\n",
            "Epoch 8, Loss: 0.8489544479052226\n",
            "Epoch 9, Loss: 0.8784996323784192\n",
            "Epoch 10, Loss: 0.8783290661374727\n",
            "Epoch 11, Loss: 0.8870744270086288\n",
            "Epoch 12, Loss: 0.8948428469896317\n",
            "Epoch 13, Loss: 0.8987029077112675\n",
            "Epoch 14, Loss: 0.9032369999587536\n",
            "Epoch 15, Loss: 0.9049132996797562\n",
            "Epoch 16, Loss: 0.9091054199139277\n",
            "Epoch 17, Loss: 0.9108297832806905\n",
            "Epoch 18, Loss: 0.9139126645276944\n",
            "Epoch 19, Loss: 0.9229602523644765\n",
            "Epoch 20, Loss: 0.9064821149408817\n",
            "Epoch 21, Loss: 0.8986120115717252\n",
            "Epoch 22, Loss: 0.9081906072298686\n",
            "Epoch 23, Loss: 0.9271186724801859\n",
            "Epoch 24, Loss: 0.9291483777264754\n",
            "Epoch 25, Loss: 0.9312358812491099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model and report the final accuracy\n",
        "def test(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "final_accuracy_CapPreDev = test(model_CapPreDev, test_loader)\n",
        "print(f\"Final Validation Accuracy: {final_accuracy_CapPreDev}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwRfWD-rPvpa",
        "outputId": "77e77bae-7396-48a0-f9ef-5c6523a0e74c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Accuracy: 81.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PresetNet_IdPreDev(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PresetNet_IdPreDev, self).__init__()\n",
        "        config = SingleRPUConfig(device=IdealizedPresetDevice())\n",
        "        self.fc1 = AnalogLinear(28 * 28, 128,rpu_config=config)  # Replace Linear with AnalogLinear\n",
        "        self.fc2 = AnalogLinear(128, 64, rpu_config=config)     # Replace Linear with AnalogLinear\n",
        "        self.fc3 = AnalogLinear(64, 10, rpu_config=config)      # Replace Linear with AnalogLinear\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RDMaDw7oQAkQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model_IdPreDev = PresetNet_IdPreDev()\n",
        "\n",
        "# Apply regroup_param_groups to the model parameters (required by the tool)\n",
        "analog_optimizer_IdPreDev = AnalogSGD(model_IdPreDev.parameters(),lr=0.01)\n",
        "analog_optimizer_IdPreDev.regroup_param_groups(model_IdPreDev)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Train the model\n",
        "train(model_IdPreDev, train_loader, criterion, analog_optimizer_IdPreDev, num_epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJu0UpVxQT3C",
        "outputId": "219f4b93-1ca9-4e55-fe1b-2c668b63cabd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.380493186612924\n",
            "Epoch 2, Loss: 0.48755881217618785\n",
            "Epoch 3, Loss: 0.3924475624660651\n",
            "Epoch 4, Loss: 0.3557643955945969\n",
            "Epoch 5, Loss: 0.33271531594296294\n",
            "Epoch 6, Loss: 0.3114937826246023\n",
            "Epoch 7, Loss: 0.2939030200615525\n",
            "Epoch 8, Loss: 0.2767156941567858\n",
            "Epoch 9, Loss: 0.26670783532162506\n",
            "Epoch 10, Loss: 0.25967533137649296\n",
            "Epoch 11, Loss: 0.24348721948141852\n",
            "Epoch 12, Loss: 0.23503106214106084\n",
            "Epoch 13, Loss: 0.22457356059302885\n",
            "Epoch 14, Loss: 0.21292693580811223\n",
            "Epoch 15, Loss: 0.20391046931346257\n",
            "Epoch 16, Loss: 0.19272308034201463\n",
            "Epoch 17, Loss: 0.18534941354145607\n",
            "Epoch 18, Loss: 0.18033011083801587\n",
            "Epoch 19, Loss: 0.17540942346677185\n",
            "Epoch 20, Loss: 0.16583890946581958\n",
            "Epoch 21, Loss: 0.16085710107038417\n",
            "Epoch 22, Loss: 0.15526536073846123\n",
            "Epoch 23, Loss: 0.151482256182159\n",
            "Epoch 24, Loss: 0.14511149413262805\n",
            "Epoch 25, Loss: 0.13906903809557358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model and report the final accuracy\n",
        "def test(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "final_accuracy_IdPreDev = test(model_IdPreDev, test_loader)\n",
        "print(f\"Final Validation Accuracy: {final_accuracy_IdPreDev}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw8t3ZGZQuIG",
        "outputId": "dc89d36a-34fe-4ce1-ca15-429cce064bbc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Accuracy: 95.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PresetNet_PCMPreDev(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PresetNet_PCMPreDev, self).__init__()\n",
        "        config = SingleRPUConfig(device=PCMPresetDevice())\n",
        "        self.fc1 = AnalogLinear(28 * 28, 128,rpu_config=config)  # Replace Linear with AnalogLinear\n",
        "        self.fc2 = AnalogLinear(128, 64, rpu_config=config)     # Replace Linear with AnalogLinear\n",
        "        self.fc3 = AnalogLinear(64, 10, rpu_config=config)      # Replace Linear with AnalogLinear\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jaOw_2CGR03d"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model_PCMPreDev = PresetNet_PCMPreDev()\n",
        "\n",
        "# Apply regroup_param_groups to the model parameters (required by the tool)\n",
        "analog_optimizer_PCMPreDev = AnalogSGD(model_PCMPreDev.parameters(),lr=0.01)\n",
        "analog_optimizer_PCMPreDev.regroup_param_groups(model_PCMPreDev)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Train the model\n",
        "train(model_PCMPreDev, train_loader, criterion, analog_optimizer_PCMPreDev, num_epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JoRnqQrSV_z",
        "outputId": "2b2603c3-c094-4b9c-ed1b-3758d9fb5c30"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.304488911231359\n",
            "Epoch 2, Loss: 2.3019596695899964\n",
            "Epoch 3, Loss: 2.301759931643804\n",
            "Epoch 4, Loss: 2.3014686632156374\n",
            "Epoch 5, Loss: 2.3014335946242013\n",
            "Epoch 6, Loss: 2.301485204696655\n",
            "Epoch 7, Loss: 2.3016721030076344\n",
            "Epoch 8, Loss: 2.3014841349919637\n",
            "Epoch 9, Loss: 2.3013617797692616\n",
            "Epoch 10, Loss: 2.3012955558300017\n",
            "Epoch 11, Loss: 2.301239277124405\n",
            "Epoch 12, Loss: 2.3012940259774526\n",
            "Epoch 13, Loss: 2.301286664009094\n",
            "Epoch 14, Loss: 2.3012045005957287\n",
            "Epoch 15, Loss: 2.301221194267273\n",
            "Epoch 16, Loss: 2.301252621014913\n",
            "Epoch 17, Loss: 2.3012128098805746\n",
            "Epoch 18, Loss: 2.3012103907267254\n",
            "Epoch 19, Loss: 2.3012085322539013\n",
            "Epoch 20, Loss: 2.301207369963328\n",
            "Epoch 21, Loss: 2.3012103895346323\n",
            "Epoch 22, Loss: 2.301208261648814\n",
            "Epoch 23, Loss: 2.301203666130702\n",
            "Epoch 24, Loss: 2.3012048637866975\n",
            "Epoch 25, Loss: 2.30120094537735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model and report the final accuracy\n",
        "def test(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "final_accuracy_PCMPreDev = test(model_PCMPreDev, test_loader)\n",
        "print(f\"Final Validation Accuracy: {final_accuracy_PCMPreDev}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmc61LeKS_0-",
        "outputId": "968d18e2-9ddd-42fb-ba56-3dbe82e78a58"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Validation Accuracy: 11.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary Table\n",
        "| Device/Model | Accuracy |\n",
        "|-----------------|-----------------|\n",
        "| Pytorch   | 96.5%   |\n",
        "| ConstantStepDevice   | 84.31%   |\n",
        "| ReRamSBPresetDevice  | 80.70%   |\n",
        "| ReRamESPresetDevice   | 11.35%   |\n",
        "| CapacitorPresetDevice   | 81.01%  |\n",
        "| IdealizedPresetDevice   | 95.48%  |\n",
        "| PCMPresetDevice   | 11.35%  |"
      ],
      "metadata": {
        "id": "nfjEl3TtYoiZ"
      }
    }
  ]
}